{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarvis/anaconda3/envs/py27/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "#Aim: To create word vectors from a game of thrones dataset\n",
    "# and analayze them to see sematic similarity\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#for word encoding\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.models.word2vec as w2v\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jarvis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jarvis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books: ['/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got1.txt', '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got2.txt', '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got3.txt', '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got4.txt', '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got5.txt']\n"
     ]
    }
   ],
   "source": [
    "print (\"Found books:\", book_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got1.txt'...\n",
      "Corpus is now 1770659 charachters loong\n",
      "\n",
      "Reading '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got2.txt'...\n",
      "Corpus is now 4071041 charachters loong\n",
      "\n",
      "Reading '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got3.txt'...\n",
      "Corpus is now 6391405 charachters loong\n",
      "\n",
      "Reading '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got4.txt'...\n",
      "Corpus is now 8107945 charachters loong\n",
      "\n",
      "Reading '/home/jarvis/My projects/Machine Learning/Siraj_Akash/word_vectors_game_of_thrones-LIVE-master/data/got5.txt'...\n",
      "Corpus is now 9719485 charachters loong\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} charachters loong\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\" , \" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Technically we tokenize sentences \n",
    "# text = u'This, is a sentence with weird» symbols… appearing everywhere¿' \n",
    "# print (mtokenizer.tokenize(text, return_str=True))\n",
    "# u'This , is a sentence with weird » symbols … appearing everywhere ¿'\n",
    "# After that we clean the tokenized sentence we spilt each word and store it into a list\n",
    "\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len (raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heraldic crest by Virginia Norey.\n",
      "[u'Heraldic', u'crest', u'by', u'Virginia', u'Norey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The book corpus contains 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print (\" The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "\n",
    "min_word_count = 3\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "context_size = 7\n",
    "\n",
    "downsampling = 1e-3\n",
    "\n",
    "seed = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 17:03:28,279 : INFO : collecting all words and their counts\n",
      "2017-03-06 17:03:28,280 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-06 17:03:28,339 : INFO : PROGRESS: at sentence #10000, processed 140984 words, keeping 10280 word types\n",
      "2017-03-06 17:03:28,381 : INFO : PROGRESS: at sentence #20000, processed 279730 words, keeping 13558 word types\n",
      "2017-03-06 17:03:28,426 : INFO : PROGRESS: at sentence #30000, processed 420336 words, keeping 16598 word types\n",
      "2017-03-06 17:03:28,463 : INFO : PROGRESS: at sentence #40000, processed 556581 words, keeping 18324 word types\n",
      "2017-03-06 17:03:28,500 : INFO : PROGRESS: at sentence #50000, processed 686247 words, keeping 19714 word types\n",
      "2017-03-06 17:03:28,541 : INFO : PROGRESS: at sentence #60000, processed 828497 words, keeping 21672 word types\n",
      "2017-03-06 17:03:28,584 : INFO : PROGRESS: at sentence #70000, processed 973830 words, keeping 23093 word types\n",
      "2017-03-06 17:03:28,635 : INFO : PROGRESS: at sentence #80000, processed 1114967 words, keeping 24252 word types\n",
      "2017-03-06 17:03:28,676 : INFO : PROGRESS: at sentence #90000, processed 1260481 words, keeping 26007 word types\n",
      "2017-03-06 17:03:28,715 : INFO : PROGRESS: at sentence #100000, processed 1393203 words, keeping 26884 word types\n",
      "2017-03-06 17:03:28,767 : INFO : PROGRESS: at sentence #110000, processed 1532150 words, keeping 27809 word types\n",
      "2017-03-06 17:03:28,810 : INFO : PROGRESS: at sentence #120000, processed 1680961 words, keeping 28486 word types\n",
      "2017-03-06 17:03:28,856 : INFO : collected 29026 word types from a corpus of 1818103 raw words and 128868 sentences\n",
      "2017-03-06 17:03:28,857 : INFO : Loading a fresh vocabulary\n",
      "2017-03-06 17:03:28,920 : INFO : min_count=3 retains 17277 unique words (59% of original 29026, drops 11749)\n",
      "2017-03-06 17:03:28,921 : INFO : min_count=3 leaves 1802699 word corpus (99% of original 1818103, drops 15404)\n",
      "2017-03-06 17:03:28,980 : INFO : deleting the raw counts dictionary of 29026 items\n",
      "2017-03-06 17:03:28,982 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-03-06 17:03:28,983 : INFO : downsampling leaves estimated 1404424 word corpus (77.9% of prior 1802699)\n",
      "2017-03-06 17:03:28,984 : INFO : estimated required memory for 17277 words and 300 dimensions: 50103300 bytes\n",
      "2017-03-06 17:03:29,048 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 17:03:31,293 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec vocabulary length:  17277\n"
     ]
    }
   ],
   "source": [
    "print(\"word2vec vocabulary length: \", len(thrones2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 17:03:33,685 : INFO : training model with 4 workers on 17277 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-03-06 17:03:33,686 : INFO : expecting 128868 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-06 17:03:34,695 : INFO : PROGRESS: at 2.41% examples, 168999 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:35,698 : INFO : PROGRESS: at 4.64% examples, 161371 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:36,724 : INFO : PROGRESS: at 6.84% examples, 155334 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:37,774 : INFO : PROGRESS: at 9.97% examples, 168393 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:38,826 : INFO : PROGRESS: at 12.55% examples, 170080 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:39,862 : INFO : PROGRESS: at 14.67% examples, 165215 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:40,891 : INFO : PROGRESS: at 17.69% examples, 170442 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:41,938 : INFO : PROGRESS: at 20.10% examples, 171193 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:42,990 : INFO : PROGRESS: at 22.18% examples, 167595 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:44,035 : INFO : PROGRESS: at 24.29% examples, 164819 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:45,050 : INFO : PROGRESS: at 26.94% examples, 165770 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:46,089 : INFO : PROGRESS: at 29.96% examples, 168669 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:47,154 : INFO : PROGRESS: at 32.11% examples, 166827 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:48,208 : INFO : PROGRESS: at 34.29% examples, 165295 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:49,264 : INFO : PROGRESS: at 36.68% examples, 164009 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:50,302 : INFO : PROGRESS: at 38.58% examples, 162528 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:51,350 : INFO : PROGRESS: at 40.52% examples, 161153 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:52,427 : INFO : PROGRESS: at 42.74% examples, 160132 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:53,511 : INFO : PROGRESS: at 44.95% examples, 159145 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:54,524 : INFO : PROGRESS: at 47.16% examples, 158466 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:55,546 : INFO : PROGRESS: at 49.40% examples, 158104 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:56,604 : INFO : PROGRESS: at 51.43% examples, 157214 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:57,615 : INFO : PROGRESS: at 53.42% examples, 156701 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:58,639 : INFO : PROGRESS: at 55.55% examples, 155831 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:03:59,693 : INFO : PROGRESS: at 56.66% examples, 152181 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:00,729 : INFO : PROGRESS: at 57.45% examples, 148618 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:01,770 : INFO : PROGRESS: at 59.37% examples, 148331 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-06 17:04:02,817 : INFO : PROGRESS: at 61.37% examples, 148001 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:03,887 : INFO : PROGRESS: at 63.64% examples, 147858 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:04,896 : INFO : PROGRESS: at 65.73% examples, 147784 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:05,957 : INFO : PROGRESS: at 68.13% examples, 147717 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:07,016 : INFO : PROGRESS: at 70.15% examples, 147431 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:08,042 : INFO : PROGRESS: at 72.19% examples, 147355 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-06 17:04:09,066 : INFO : PROGRESS: at 74.27% examples, 147173 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:10,104 : INFO : PROGRESS: at 76.53% examples, 147017 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-06 17:04:11,144 : INFO : PROGRESS: at 78.56% examples, 147012 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:12,180 : INFO : PROGRESS: at 80.50% examples, 146847 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:13,223 : INFO : PROGRESS: at 82.48% examples, 146500 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:14,292 : INFO : PROGRESS: at 84.71% examples, 146437 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:15,343 : INFO : PROGRESS: at 87.03% examples, 146459 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:16,355 : INFO : PROGRESS: at 89.06% examples, 146244 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:17,383 : INFO : PROGRESS: at 91.21% examples, 146347 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:18,462 : INFO : PROGRESS: at 93.22% examples, 146139 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-06 17:04:19,524 : INFO : PROGRESS: at 95.54% examples, 146072 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-06 17:04:20,577 : INFO : PROGRESS: at 97.76% examples, 146080 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-06 17:04:21,608 : INFO : PROGRESS: at 99.67% examples, 145982 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-06 17:04:21,611 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-06 17:04:21,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-06 17:04:21,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-06 17:04:21,747 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-06 17:04:21,748 : INFO : training on 9090515 raw words (7021536 effective words) took 48.1s, 146103 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7021536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 17:05:47,173 : INFO : saving Word2Vec object under trained/thrones2vec.w2v, separately None\n",
      "2017-03-06 17:05:47,175 : INFO : not storing attribute syn0norm\n",
      "2017-03-06 17:05:47,176 : INFO : not storing attribute cum_table\n",
      "2017-03-06 17:05:47,724 : INFO : saved trained/thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\",\"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 17:05:53,313 : INFO : loading Word2Vec object from trained/thrones2vec.w2v\n",
      "2017-03-06 17:05:53,479 : INFO : loading wv recursively from trained/thrones2vec.w2v.wv.* with mmap=None\n",
      "2017-03-06 17:05:53,479 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-06 17:05:53,480 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-06 17:05:53,481 : INFO : loaded trained/thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = thrones2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[thrones2vec.vocab[word].index])\n",
    "            for word in thrones2vec.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
